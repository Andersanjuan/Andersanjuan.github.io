<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RAG Clinical Assistant API</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div id="container">

    <header id="top">
      <div class="intro">
        <h1>RAG Clinical Assistant API (Grounded RAG)</h1>
        <p>
          A grounded Retrieval-Augmented Generation backend for clinical/biomedical text. The system retrieves
          evidence from a vector database, enforces distance-based abstention, and validates citations to reduce
          hallucinations in high-stakes settings.
        </p>

        <p class="links">
          <a href="index.html">Home</a> 路
          <a href="portfolio.html">Portfolio</a> 路
          <a href="blog.html">Blog</a> 路
          <a href="maker-space.html">Maker Space</a> 路
          <a href="notable-people.html">Notable People</a>
        </p>
      </div>
    </header>

    <main>
      <h2>Project Summary</h2>
      <p>
        This project demonstrates an end-to-end RAG pipeline with grounding diagnostics and secure API access.
        It is designed as an internal-facing backend service (FastAPI) with Docker deployment support.
      </p>

      <h2>Key Features</h2>
      <ul>
        <li>RAG ingestion pipeline (chunking, embeddings, ChromaDB storage)</li>
        <li>Evidence retrieval with similarity distances exposed to clients</li>
        <li>Grounding gate: abstains when retrieval confidence is low</li>
        <li>Citation enforcement for non-abstained answers</li>
        <li>API-key authentication and privacy-aware logging</li>
        <li>Dockerized deployment</li>
      </ul>

      <h2>Links</h2>
      <p>
        <a href="https://github.com/Andersanjuan/grounded-clinical-rag-api" target="_blank" rel="noopener noreferrer">
          View code on GitHub
        </a>
      </p>
    

      <h2>System Walkthrough</h2>
      <p>
        The following steps illustrate the end-to-end workflow of the MedRAG system,
        from ingestion to grounded answering and evaluation.
      </p>

      <div class="workflow">

        <section class="workflow-step compact">
          <h3>1. API Interface</h3>
          <p>
            The system exposes retrieval and question-answering endpoints through a FastAPI
            backend with automatically generated OpenAPI documentation.
          </p>
          <img src="assets/medrag/01-api-docs.png" alt="FastAPI Swagger UI showing retrieve and query endpoints">
        </section>

        <section class="workflow-step">
          <h3>2. Document Ingestion and Embedding</h3>
          <p>
            Clinical and biomedical documents are loaded, chunked, embedded using
            sentence-transformer models, and stored in a Chroma vector database.
          </p>
          <img src="assets/medrag/02-ingestion.png" alt="Terminal output showing document ingestion and embedding">
        </section>

        <section class="workflow-step compact">
          <h3>3. Evidence Retrieval</h3>
          <p>
            For a given query, the system retrieves the most relevant text chunks and
            reports similarity distances to support transparency and interpretability.
          </p>
          <img src="assets/medrag/03-retrieve.png" alt="Retrieve endpoint returning chunks with similarity distances">
        </section>

        <section class="workflow-step compact">
          <h3>4. Grounded Question Answering</h3>
          <p>
            When retrieval confidence is sufficient, the language model generates an answer
            grounded in the retrieved context and returns explicit citations.
          </p>
          <img src="assets/medrag/04-query-answer.png" alt="Query endpoint answering with citations and grounding diagnostics">
        </section>

        <section class="workflow-step compact">
          <h3>5. Safe Abstention for Out-of-Scope Queries</h3>
          <p>
            If retrieval confidence falls below a defined threshold, the system abstains
            from answering to avoid hallucinated responses in medical contexts.
          </p>
          <img src="assets/medrag/05-query-abstain.png" alt="Query endpoint abstaining due to low retrieval confidence">
        </section>

        <section class="workflow-step">
          <h3>6. Evaluation and Validation</h3>
          <p>
            An HTTP-based evaluation harness tests in-scope and out-of-scope questions,
            verifying abstention behavior and citation enforcement.
          </p>
          <img src="assets/medrag/06-evaluation.png" alt="Evaluation script output summarizing grounding behavior">
        </section>

      </div>

    </main>

  </div>
</body>
</html>
